Date: 11 Dec 2018
Summary of "Deep Learning with PyTorch: A 60 Minute Blitz" 
https://seba-1511.github.io/tutorials/beginner/deep_learning_60min_blitz.html

Basics:
1. Converting `torch` to/from `numpy` using `from_numpy()` and `numpy()`
2. Operations (`+,-,/,*`) syntax 

Autograd:
1. (Class) `autograd.Variable`: Central class that wraps a Tensor and equips it with operations defined on it
  - The raw tensor is accessed via the `.data` attribute
  - The gradient wrt this variable is accumulated into `.grad`
2. (Class) `Function`: Every variable has a `.grad_fn` attribute that references a Function that has created the `Variable`
3. To compute derivatives on a `Variable`, call `.backward()` on the `Variable`
  - if `Variable` is NOT a scalar, i.e. holds more than one element, you need to specify a `grad_output` argument that is a tensor of matching shape.
4. Include the parameter `requires_grad = True` while initializing the `Variable`

Neural Networks:
1. `nn.Module`: A convenient way of encapsulating parameters, with helpers for moving them to GPU
2. `nn.Parameter`: A kind of Variable that is automatically registered as a parameter when assigned as an attribute to a `Module`
3. The model should contain the methods `.__init__()` and `.forward()`. Initialize the layers in the `.__init__()` method, and specify the order of the operations in the `.forward()` method.

Loss Functions:
1. There are several different loss functions, which can be accessed through the `nn` package.
2. When we call `loss.backward()`, the whole graph is differentiated wrt the loss, and all Variables in the graph will have their `.grad` Variable (see above) accumulated with the gradient.
3. Before calling $loss.backward()`, zero out the gradient buffers of all parameters by calling `.zero_grad()` on either the model or the optimizer.
4. Usually, we just give the loss function 2 parameters: the label and the prediction.

Optimizer:
1. Accessible through the package `torch.optim`.
2. Pass in the parameters of the model, e.g. `model.parameters()` as well as a learning rate. 

Bringing it all together:
1. Create a `model`
2. Create an `optimizer`
3. Specify a loss function `loss_fn`
4. For each batch,
- Zero out the gradient buffers
- Calculate the prediction
- Calculate the loss using the prediction and true label
- Call `.backward()` on the loss 
- Call `.step()` on the `optimizer`

------------------------------------------------------------------------------------------------

Loading Data:
1. `torchvision` is a package that has data loaders for common datasets (MNIST/CIFAR10 etc), as well as data transformers for images.
2.  Use `torch.utils.data.DataLoader` to create a list of training/testing data that we can enumerate over during training/testing.
