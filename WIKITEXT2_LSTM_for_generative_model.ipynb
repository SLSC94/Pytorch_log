{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Employing a LSTM to create a generative NLP model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook used the following sources:\n",
    "\n",
    "#### 1) http://mlexplained.com/2018/02/15/language-modeling-tutorial-in-torchtext-practical-torchtext-part-2/\n",
    "#### 2) http://www.jessicayung.com/lstms-for-time-series-in-pytorch/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd\n",
    "from torch.autograd import Variable as V\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext.datasets import WikiText2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field()\n",
    "LABEL = data.Field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = WikiText2.splits(TEXT) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, vectors= torchtext.vocab.GloVe(name='6B', dim=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BPTT helps us to feed the inputs in a staggered way (since we want text generation here)\n",
    "train_iter, valid_iter, test_iter = torchtext.data.BPTTIterator.splits(batch_size=30, #number of sentences\n",
    "                                                                        bptt_len=10, #max length of a sentence\n",
    "                                                                        datasets=(train,valid,test) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a generative model using LSTM cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Model(nn.Module):\n",
    "    def __init__(self, in_dim, num_vocab, hid_dim, n_layer, batch_size):\n",
    "        super().__init__()\n",
    "        #store necessary parameters\n",
    "        self.in_dim = in_dim\n",
    "        self.num_vocab = num_vocab\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layer = n_layer \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        #initialize hidden parameters\n",
    "        self.init_hidden()\n",
    "        \n",
    "        #define the encoder, decoder, and the LSTM cell. \n",
    "        self.encoder = nn.Embedding(embedding_dim=in_dim, num_embeddings=num_vocab)\n",
    "        self.decoder = nn.Linear(in_features=hid_dim, out_features=num_vocab)\n",
    "        self.RNN = nn.LSTM(input_size=in_dim, hidden_size=hid_dim)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.encoder(input)\n",
    "        output, self.hidden = self.RNN(output, self.hidden)\n",
    "        #reshape so we have shape (batch_size, hidden_dim)\n",
    "        decoded = self.decoder(output.view(-1, output.size(2)) )\n",
    "        #reshape again: Although this is not really necessary, it's good practice to do so.\n",
    "        return decoded.reshape(output.size(0),output.size(1), decoded.size(1))\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        self.hidden = (torch.zeros(self.n_layer, self.batch_size, self.hid_dim), \n",
    "                       torch.zeros(self.n_layer, self.batch_size, self.hid_dim) )\n",
    "    \n",
    "    def refresh_hidden(self):\n",
    "        self.hidden = tuple(V(v.data) for v in self.hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight_matrix contains the word embeddings that we pass on to the encoder in our model\n",
    "weight_matrix = TEXT.vocab.vectors\n",
    "model = RNN_Model(batch_size=30,hid_dim=100,in_dim=weight_matrix.size(1)\n",
    "                  ,n_layer=1,num_vocab=weight_matrix.size(0))\n",
    "\n",
    "model.encoder.weight.data.copy_(weight_matrix)\n",
    "\n",
    "\n",
    "# initialize loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, betas=(0.7, 0.99))\n",
    "n_tokens = weight_matrix.size(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainepochs(epochs):\n",
    "    for j in range(epochs):\n",
    "        Loss = 0\n",
    "        for i,batch in enumerate(train_iter):\n",
    "            \n",
    "            # zero gradient\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # use the old hidden & cell vector without backpropogating all the way back\n",
    "            model.refresh_hidden()\n",
    "            \n",
    "            #load in the text and target\n",
    "            text, target = batch.text, batch.target\n",
    "            \n",
    "            prediction = model(text)\n",
    "            \n",
    "            loss = criterion(prediction.view(-1, prediction.size(2)), target.view(-1) )\n",
    "        \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            Loss = Loss + loss.item()\n",
    "            # Uncomment if you want to see the loss\n",
    "            # if i % 100 == 99:\n",
    "            #     print(\"Batch: \", i, \" Loss: \", Loss/(i * 300))\n",
    "\n",
    "        Loss = Loss/(len(train.examples[0].text))\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        for batch in valid_iter:\n",
    "            model.refresh_hidden()\n",
    "            text, targets = batch.text, batch.target\n",
    "            prediction = model(text)\n",
    "            loss = criterion(prediction.view(-1, n_tokens), targets.view(-1))\n",
    "            val_loss = val_loss + loss.item()\n",
    "        val_loss /= len(valid.examples[0].text)\n",
    "        print('Epoch:', j, ', Training Loss:', Loss, ', Validation Loss: ', val_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "trainepochs(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we print out the words that have been trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_ids_to_sentence(id_tensor, vocab, join=None):\n",
    "    \"\"\"Converts a sequence of word ids to a sentence\"\"\"\n",
    "    if isinstance(id_tensor, torch.LongTensor):\n",
    "        ids = id_tensor.transpose(0, 1).contiguous().view(-1)\n",
    "    elif isinstance(id_tensor, np.ndarray):\n",
    "        ids = id_tensor.transpose().reshape(-1)\n",
    "    batch = [vocab.itos[ind] for ind in ids] # denumericalize\n",
    "    if join is None:\n",
    "        return batch\n",
    "    else:\n",
    "        return join.join(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrs = model(b.text).cpu().data.numpy()\n",
    "word_ids_to_sentence(np.argmax(arrs, axis=2), TEXT.vocab, join=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
